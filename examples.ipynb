{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e64923d-b5fe-4255-bab9-560fb126b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install editable version of this repository\n",
    "!python -m pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d659b989-3e26-47fe-b0e9-eabc091e7b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'activation_extractor' from '/orfeo/cephfs/scratch/area/evillegas/glm/activation-extractor/src/activation_extractor/__init__.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "# Import Pytorch & Huggingface ==========================\n",
    "#set cache dir before imports \n",
    "functions_path=\"/orfeo/scratch/area/evillegas/glm/glm_analysis\"\n",
    "huggingface_path=\"/orfeo/cephfs/scratch/area/evillegas/.cache/\"\n",
    "environment_path=\"/orfeo/cephfs/scratch/area/evillegas/glm/dgxtorch/bin/activate\"\n",
    "home_path=\"/orfeo/scratch/area/evillegas/\"\n",
    "\n",
    "\n",
    "from configuration import huggingface_path as cache_dir\n",
    "#cache_dir = cache_path_here\n",
    "os.environ['HF_HOME'] = f\"{cache_dir}/huggingface\"\n",
    "os.environ['TRANSFORMERS_CACHE']= f\"{cache_dir}/huggingface/hub\"\n",
    "os.environ['TORCH_HOME']= f\"{cache_dir}/torch\"\n",
    "os.environ['XDG_CACHE_HOME']= cache_dir\n",
    "\n",
    "#imports \n",
    "import torch\n",
    "from torch.cuda import memory_allocated, empty_cache\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import local functions ==================================\n",
    "import activation_extractor\n",
    "from activation_extractor import *\n",
    "reload(activation_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27def865-badc-4403-b974-204c111692b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'activation_extractor.model_functions.default_hooked_layers' from '/orfeo/cephfs/scratch/area/evillegas/glm/activation-extractor/src/activation_extractor/model_functions/default_hooked_layers.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from activation_extractor.extractors import intermediateExtractorBase \n",
    "reload(intermediateExtractorBase)\n",
    "from activation_extractor.inferencers import inferencerBase\n",
    "reload(inferencerBase)\n",
    "from activation_extractor.model_functions import load_models, tokenize_funs, inference_funs, default_hooked_layers\n",
    "reload(load_models)\n",
    "reload(tokenize_funs)\n",
    "reload(inference_funs)\n",
    "reload(default_hooked_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80040e65-180b-4ec5-be6f-d834e7d18213",
   "metadata": {},
   "source": [
    "# Proteins/DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "049ffd11-5067-4722-ab47-f56ebe585b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "sequences = [\"AAAAAAAAAAA\", \"CCCCCCCCCC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2bcca01-501a-45f3-aae5-53056f529af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "inferencer = activation_extractor.Inferencer(model_name, device='cuda', half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c927f6ad-26ce-42ba-983b-c8c162657c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate activation extractor\n",
    "layers_to_hook = activation_extractor.get_layers_to_hook(inferencer.model,inferencer.model_type)\n",
    "extractor = activation_extractor.IntermediateExtractor(inferencer.model, layers_to_hook)\n",
    "extractor.register_hooks()\n",
    "\n",
    "#inference\n",
    "processed = inferencer.process(sequences) #tokenize\n",
    "outputs = inferencer.inference(processed)\n",
    "\n",
    "#extractor outputs\n",
    "#extractor.save_outputs('results/embeddings/test')\n",
    "extractor.clear_all_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ca1dd8-5d55-4323-a2e2-61233ad04ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor.get_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b25a5-e2f7-4b95-abb5-3d863e66abf5",
   "metadata": {},
   "source": [
    "# Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b86a6bf-00eb-4c8c-9b0c-72be443f5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34fe08e-4270-4279-9973-eaeb190f605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"timm/vgg16.tv_in1k\"\n",
    "inferencer = activation_extractor.Inferencer(model_name, device='cuda', half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42197cb2-8548-417c-8b92-b9a6ce5c5027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#intermediate activation extractor\n",
    "layers_to_hook = default_hooked_layers.get_layers_to_hook(inferencer.model,inferencer.model_type)\n",
    "extractor = intermediateExtractor.IntermediateExtractorBase(inferencer.model, layers_to_hook)\n",
    "extractor.register_hooks()\n",
    "\n",
    "#inference\n",
    "processed = inferencer.process(image)\n",
    "outputs = inferencer.inference(processed)\n",
    "\n",
    "#extractor outputs\n",
    "#extractor.save_outputs('results/embeddings/test')\n",
    "extractor.clear_all_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4548feef-b365-49de-b4fd-6f0700bb4acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor.get_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb57c03-d901-40ff-b877-b0bf0a0baff7",
   "metadata": {},
   "source": [
    "# Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "678b482e-d287-4d2b-bbe6-c52845990bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "\n",
    "input_data = {\"text\":text,\n",
    "             \"image\":image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f33dc55e-9506-4e38-b108-6ba003cd11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "inferencer = activation_extractor.Inferencer(model_name, device='cuda', half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df6a2c63-bad0-495f-8951-00556b0582b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate activation extractor\n",
    "layers_to_hook = default_hooked_layers.get_layers_to_hook(inferencer.model,inferencer.model_type)\n",
    "extractor = intermediateExtractor.IntermediateExtractorBase(inferencer.model, layers_to_hook)\n",
    "extractor.register_hooks()\n",
    "\n",
    "#inference\n",
    "processed = inferencer.process(input_data)\n",
    "outputs = inferencer.inference(processed)\n",
    "\n",
    "#extractor outputs\n",
    "#extractor.save_outputs('results/embeddings/test')\n",
    "extractor.clear_all_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d5a68f-986f-48da-bddd-cbf2b6e6ba39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor.get_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1483d3-2e8c-40f5-8937-20330a2f4166",
   "metadata": {},
   "source": [
    "# Inference Over a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6a8238-ab54-472e-a9fd-23d894663e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_extractor.scripts.inference import main_inference, load_the_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b99e0f4-600c-4cbe-8c97-b21e873ab0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "output_folder=\"test\"\n",
    "emb_format=\"mean\"\n",
    "save_method=\"numpy\"\n",
    "max_batches=1\n",
    "\n",
    "data_args = {\n",
    "        \"data_type\":\"dna\",\n",
    "        \"target_col\":\"sequence\",\n",
    "        \"batch_size\":4,\n",
    "        \"data_source\":\"huggingface\",\n",
    "        \"dataset_name\":\"InstaDeepAI/human_reference_genome\",\n",
    "        \"dataset_partition\":\"validation\",\n",
    "        # \"data_source\":\"local\",\n",
    "        # \"input_path\":\"test.csv\",\n",
    "        \"max_length\":999,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5b943d-29da-478e-8e6c-62b9c7ca2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"facebook/convnext-tiny-224\"\n",
    "output_folder=\"test\"\n",
    "emb_format=\"mean\"\n",
    "save_method=\"numpy\"\n",
    "max_batches=1\n",
    "\n",
    "data_args = {\n",
    "        \"data_type\":\"image\",\n",
    "        \"target_col\":\"img\",\n",
    "        \"batch_size\":1,\n",
    "        \"data_source\":\"huggingface\",\n",
    "        \"dataset_name\":\"uoft-cs/cifar100\",\n",
    "        \"dataset_partition\":\"train\",\n",
    "        # \"data_source\":\"local\",\n",
    "        # \"input_path\":\"test.csv\",\n",
    "        \"max_length\":999,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cf19605-8978-4d64-b79f-a31a2dd9cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_pil(batch):\n",
    "    \"\"\"\n",
    "    Convert PIL images to numpy arrays.\n",
    "    \"\"\"\n",
    "    batch = [np.array(img) for img in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f42446-953d-4e33-8f31-ea8184c7c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [ np.array(dataset[\"train\"][\"img\"][i]) for i in range(10) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7169b68-643b-46c8-829d-1428e8a35e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49c0cb2d-6f19-4937-9ee0-a54d76dfe23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65508f08-305e-4d22-80d6-9a141ec40331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # Normalize the images\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR100(\n",
    "    root='./data',  # Directory to store the dataset\n",
    "    train=True,  # Download the training dataset\n",
    "    download=True,  # Download if not already downloaded\n",
    "    transform=transform  # Apply transformations\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,  # Number of samples per batch\n",
    "    shuffle=True,  # Shuffle the dataset\n",
    "    num_workers=2,  # Number of subprocesses to use for data loadingt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18357c87-50c0-4f2e-90fb-80238e770017",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_dataset, batch_size=2, \n",
    "                             shuffle=False, collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "823a8310-3ef3-4731-b82c-9b9b49645ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"uoft-cs/cifar100\")\n",
    "data_loader = DataLoader(dataset[\"train\"], batch_size=2, \n",
    "                             shuffle=False, collate_fn=collate_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9084975e-1fed-4121-a84e-bee17a2de07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = load_the_data(**data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e9b0dfb-2617-4495-ba42-04ac8d451238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 0.2153,  0.5819,  0.2593,  ...,  1.3588,  1.3002,  1.1829],\n",
       "           [ 0.1714,  0.4646,  0.2447,  ...,  1.2269,  1.0656,  1.1096],\n",
       "           [ 0.0101, -0.0339, -0.1072,  ...,  0.5965,  0.6552,  0.9630],\n",
       "           ...,\n",
       "           [-1.1480, -1.1334, -1.2360,  ..., -1.2507, -1.1041, -1.0894],\n",
       "           [-1.1774, -1.1627, -1.2653,  ..., -1.2067, -1.0747, -0.6496],\n",
       "           [-1.2213, -1.2213, -1.2946,  ..., -0.1951,  0.3473,  1.0510]],\n",
       " \n",
       "          [[-0.1240,  0.4417,  0.0901,  ...,  1.4661,  1.4966,  1.2979],\n",
       "           [-0.1393,  0.2735,  0.0442,  ...,  1.2979,  1.1297,  1.1603],\n",
       "           [-0.2157, -0.3074, -0.3380,  ...,  0.3958,  0.4111,  0.7781],\n",
       "           ...,\n",
       "           [-1.2706, -1.2553, -1.3624,  ..., -1.4235, -1.4082, -1.4541],\n",
       "           [-1.3165, -1.2859, -1.3776,  ..., -1.4694, -1.4388, -0.8731],\n",
       "           [-1.3471, -1.3471, -1.4235,  ..., -0.3686,  0.3041,  1.2214]],\n",
       " \n",
       "          [[-0.0199,  0.4914,  0.2073,  ...,  1.5708,  1.5992,  1.3436],\n",
       "           [-0.0483,  0.3493,  0.1647,  ...,  1.4004,  1.1874,  1.1874],\n",
       "           [-0.0768, -0.1762, -0.2472,  ...,  0.4914,  0.4772,  0.8181],\n",
       "           ...,\n",
       "           [-0.8579, -0.8437, -1.0142,  ..., -1.0994, -1.0994, -1.1136],\n",
       "           [-0.9006, -0.8864, -1.0426,  ..., -1.1136, -1.1136, -0.6307],\n",
       "           [-0.9716, -0.9716, -1.0426,  ..., -0.1336,  0.4914,  1.2868]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2300,  0.2447,  0.3033,  ...,  0.4499,  0.4353,  0.4792],\n",
       "           [ 0.3766,  0.3913,  0.4206,  ...,  0.5672,  0.5232,  0.5525],\n",
       "           [ 0.5232,  0.5232,  0.5525,  ...,  0.6698,  0.5819,  0.5965],\n",
       "           ...,\n",
       "           [ 1.0510,  1.0950,  1.1536,  ...,  0.9337,  1.4175,  1.5054],\n",
       "           [ 1.1536,  1.0950,  1.1096,  ...,  1.3735,  1.4468,  1.4175],\n",
       "           [ 0.9777,  1.0510,  1.2416,  ...,  1.3588,  1.3442,  1.3442]],\n",
       " \n",
       "          [[ 1.0686,  1.0838,  1.1450,  ...,  1.2826,  1.2673,  1.2214],\n",
       "           [ 1.1756,  1.1756,  1.2214,  ...,  1.3285,  1.3132,  1.2673],\n",
       "           [ 1.2520,  1.2367,  1.2826,  ...,  1.3438,  1.2979,  1.2826],\n",
       "           ...,\n",
       "           [ 1.2520,  1.2826,  1.3285,  ...,  0.9921,  1.4966,  1.5884],\n",
       "           [ 1.2214,  1.1909,  1.2062,  ...,  1.4814,  1.5578,  1.4966],\n",
       "           [ 1.0991,  1.1909,  1.3896,  ...,  1.5272,  1.4814,  1.4355]],\n",
       " \n",
       "          [[ 1.5282,  1.5424,  1.5850,  ...,  1.5850,  1.6419,  1.6277],\n",
       "           [ 1.6561,  1.6419,  1.6845,  ...,  1.6703,  1.7129,  1.6987],\n",
       "           [ 1.6845,  1.6703,  1.7129,  ...,  1.6845,  1.6987,  1.6987],\n",
       "           ...,\n",
       "           [ 1.1874,  1.2158,  1.2584,  ...,  0.8891,  1.3720,  1.4856],\n",
       "           [ 1.1874,  1.1731,  1.2016,  ...,  1.3578,  1.4430,  1.4288],\n",
       "           [ 1.0453,  1.1305,  1.3152,  ...,  1.4288,  1.3862,  1.3720]]],\n",
       " \n",
       " \n",
       "         [[[-0.4737, -0.5030, -0.5030,  ...,  0.1274,  1.1976,  0.8751],\n",
       "           [-0.5030, -0.5177, -0.5177,  ...,  0.2007,  1.0656,  1.4468],\n",
       "           [-0.5030, -0.5177, -0.5177,  ..., -0.3124,  0.8897,  1.4028],\n",
       "           ...,\n",
       "           [ 0.4206,  0.3473,  0.2740,  ...,  0.2007,  0.2886,  0.3913],\n",
       "           [ 0.2300,  0.1420,  0.0687,  ...,  0.2007,  0.4792,  0.7138],\n",
       "           [ 0.6991,  0.6405,  0.6258,  ...,  0.6845,  0.7285,  0.8164]],\n",
       " \n",
       "          [[-1.1942, -1.1942, -1.1942,  ..., -0.8120,  0.4111,  0.4723],\n",
       "           [-1.1942, -1.2095, -1.2095,  ..., -0.7661,  0.1512,  0.9462],\n",
       "           [-1.1942, -1.2095, -1.2095,  ..., -1.1177, -0.1393,  0.6099],\n",
       "           ...,\n",
       "           [-0.0628, -0.1393, -0.1698,  ..., -0.1851, -0.1240, -0.0781],\n",
       "           [-0.2310, -0.2769, -0.2769,  ..., -0.2463, -0.1087,  0.0289],\n",
       "           [ 0.2277,  0.1818,  0.1665,  ...,  0.2124,  0.2430,  0.2888]],\n",
       " \n",
       "          [[-1.0142, -1.0142, -1.0142,  ..., -0.9006, -0.1762,  0.0937],\n",
       "           [-1.0142, -1.0284, -1.0284,  ..., -0.9574, -0.4318,  0.3209],\n",
       "           [-1.0284, -1.0284, -1.0284,  ..., -1.0426, -0.6733, -0.0483],\n",
       "           ...,\n",
       "           [-1.4403, -1.4403, -1.4545,  ..., -1.4687, -1.3977, -1.3835],\n",
       "           [-1.4829, -1.5113, -1.5397,  ..., -1.3977, -1.3125, -1.3267],\n",
       "           [-1.5539, -1.5539, -1.5681,  ..., -1.5113, -1.5113, -1.4971]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.3417, -0.2391, -0.1218,  ...,  0.3913,  0.3766,  0.3326],\n",
       "           [-0.3271, -0.2831, -0.2098,  ...,  0.3033,  0.3180,  0.3033],\n",
       "           [-0.2831, -0.1805, -0.1072,  ...,  0.1714,  0.2153,  0.1860],\n",
       "           ...,\n",
       "           [-1.6172, -1.6465, -1.6318,  ..., -1.3826, -1.3826, -1.3973],\n",
       "           [-1.6611, -1.6465, -1.6172,  ..., -1.4119, -1.4119, -1.4119],\n",
       "           [-1.6905, -1.6465, -1.6025,  ..., -1.4266, -1.4412, -1.4412]],\n",
       " \n",
       "          [[ 0.5487,  0.6252,  0.7169,  ...,  1.0380,  1.0227,  0.9768],\n",
       "           [ 0.5793,  0.5793,  0.6252,  ...,  0.9462,  0.9615,  0.9462],\n",
       "           [ 0.6252,  0.6863,  0.7322,  ...,  0.8239,  0.8698,  0.8392],\n",
       "           ...,\n",
       "           [-1.2859, -1.3012, -1.2706,  ..., -0.9343, -0.8884, -0.8884],\n",
       "           [-1.3471, -1.3471, -1.3012,  ..., -0.9954, -0.9496, -0.9801],\n",
       "           [-1.3776, -1.3471, -1.3012,  ..., -1.0413, -1.0260, -1.0719]],\n",
       " \n",
       "          [[ 1.1874,  1.2158,  1.3010,  ...,  1.4856,  1.4714,  1.4430],\n",
       "           [ 1.2016,  1.1874,  1.2300,  ...,  1.4146,  1.4288,  1.4146],\n",
       "           [ 1.2442,  1.2868,  1.3152,  ...,  1.3010,  1.3436,  1.3152],\n",
       "           ...,\n",
       "           [-0.5313, -0.5739, -0.5597,  ..., -0.2898, -0.2472, -0.2188],\n",
       "           [-0.6023, -0.6165, -0.5881,  ..., -0.3324, -0.3040, -0.3040],\n",
       "           [-0.6449, -0.6165, -0.5739,  ..., -0.3608, -0.3466, -0.3750]]],\n",
       " \n",
       " \n",
       "         [[[ 0.4059,  0.3619,  0.3033,  ...,  0.2886,  0.2886,  0.2740],\n",
       "           [ 0.4499,  0.5086,  0.4939,  ...,  0.2886,  0.2886,  0.2886],\n",
       "           [ 0.3473,  0.4206,  0.4206,  ..., -0.0046,  0.0101,  0.0101],\n",
       "           ...,\n",
       "           [-0.9135, -0.9428, -0.9428,  ...,  0.5379,  0.0101, -0.4004],\n",
       "           [-0.8695, -0.8988, -0.8988,  ...,  0.1420,  0.1420, -0.4004],\n",
       "           [-0.7962, -0.8255, -0.8402,  ...,  0.1127,  0.2300, -0.2538]],\n",
       " \n",
       "          [[ 0.4111,  0.3194,  0.1818,  ...,  0.3347,  0.3347,  0.3194],\n",
       "           [ 0.5793,  0.5946,  0.5334,  ...,  0.2735,  0.2735,  0.2735],\n",
       "           [ 0.5334,  0.5793,  0.5640,  ..., -0.0322, -0.0169, -0.0169],\n",
       "           ...,\n",
       "           [-0.2769, -0.2769, -0.2616,  ...,  0.8392,  0.7169,  0.3958],\n",
       "           [-0.2616, -0.2769, -0.2463,  ...,  0.3194,  0.7628,  0.3958],\n",
       "           [-0.2463, -0.2463, -0.2463,  ...,  0.3194,  0.6710,  0.5029]],\n",
       " \n",
       "          [[ 0.2215,  0.1221, -0.0483,  ...,  0.0653,  0.0511,  0.0369],\n",
       "           [ 0.4630,  0.4630,  0.3636,  ..., -0.3466, -0.3324, -0.3182],\n",
       "           [ 0.3636,  0.4488,  0.5056,  ..., -1.2414, -1.1704, -1.1846],\n",
       "           ...,\n",
       "           [-0.5171, -0.4602, -0.4176,  ...,  0.2215,  0.3920, -0.3040],\n",
       "           [-0.4745, -0.4176, -0.3608,  ..., -0.6449,  0.5766, -0.2046],\n",
       "           [-0.4460, -0.3892, -0.3466,  ..., -1.2698,  0.3920,  0.2073]]],\n",
       " \n",
       " \n",
       "         [[[-0.5763, -0.6643, -0.6643,  ..., -1.5878, -1.4412, -1.7344],\n",
       "           [-0.5323, -0.5910, -0.6496,  ..., -1.4412, -1.1627, -1.6465],\n",
       "           [-0.5323, -0.5763, -0.5763,  ..., -1.0014, -0.3124, -0.4590],\n",
       "           ...,\n",
       "           [-0.1218, -0.0632, -0.1512,  ...,  0.7285,  0.7578,  0.6845],\n",
       "           [-0.1365, -0.0046,  0.1127,  ...,  0.5672,  0.5819,  0.7431],\n",
       "           [ 0.1274,  0.1274,  0.4499,  ...,  0.2740,  0.3619,  0.6552]],\n",
       " \n",
       "          [[-1.2859, -1.3624, -1.3624,  ..., -1.7599, -1.6070, -1.7752],\n",
       "           [-1.2706, -1.3165, -1.3776,  ..., -1.6528, -1.2859, -1.6070],\n",
       "           [-1.3165, -1.3624, -1.3624,  ..., -1.3318, -0.5673, -0.5062],\n",
       "           ...,\n",
       "           [-1.1177, -1.0107, -1.1942,  ..., -0.6591, -0.6132, -0.6744],\n",
       "           [-1.0260, -0.8120, -0.8273,  ..., -0.6897, -0.6897, -0.6285],\n",
       "           [-0.6132, -0.5215, -0.3686,  ..., -0.8425, -0.7661, -0.6744]],\n",
       " \n",
       "          [[-1.2272, -1.2983, -1.2983,  ..., -1.4829, -1.3551, -1.5255],\n",
       "           [-1.1988, -1.2414, -1.2983,  ..., -1.3977, -1.0710, -1.3835],\n",
       "           [-1.2272, -1.2556, -1.2698,  ..., -1.0568, -0.3182, -0.2898],\n",
       "           ...,\n",
       "           [-1.1704, -1.0852, -1.2130,  ..., -0.6165, -0.5739, -0.6307],\n",
       "           [-1.0284, -0.8295, -0.8579,  ..., -0.6165, -0.6307, -0.5881],\n",
       "           [-0.5739, -0.4745, -0.3892,  ..., -0.7727, -0.7443, -0.6449]]]]),\n",
       " tensor([19, 71, 98, 46, 46, 57, 18, 73, 95, 63, 60, 56,  7, 63, 20, 34,  2, 19,\n",
       "         36, 91, 30, 15, 77, 70, 44, 30, 27, 62, 23, 86, 26, 73, 59, 22,  4, 33,\n",
       "         18, 64, 38, 53, 80, 97, 76, 69, 14, 59, 75,  8, 73, 43, 22, 91, 48, 53,\n",
       "          3, 34, 83, 40, 65, 96, 81, 30,  1,  8])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbb3c80-f9cf-4c2c-a8a7-de23aa879617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder is: test/facebook/convnext-tiny-224/mean\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'NoneType'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/orfeo/cephfs/scratch/area/evillegas/glm/activation-extractor/src/activation_extractor/scripts/inference.py:207\u001b[0m, in \u001b[0;36mmain_inference\u001b[0;34m(model_name, output_folder, emb_format, save_method, max_batches, data_args)\u001b[0m\n\u001b[1;32m    202\u001b[0m start_total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):   \n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m### Inference Part ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m#process\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43minferencer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m#inference\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/orfeo/cephfs/scratch/area/evillegas/glm/activation-extractor/src/activation_extractor/inferencers/inferencerBase.py:72\u001b[0m, in \u001b[0;36mInferencerBase.process\u001b[0;34m(self, input_data, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    Process input data (tokenize or image processing).\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    The tokenizer works with batches of sequences (list of strings).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :return: the processed inputs\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     processed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_input\n",
      "File \u001b[0;32m/orfeo/cephfs/scratch/area/evillegas/glm/activation-extractor/src/activation_extractor/model_functions/process_funs.py:22\u001b[0m, in \u001b[0;36mdefine_process_function.<locals>.process_fun\u001b[0;34m(image, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_fun\u001b[39m(image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed\n",
      "File \u001b[0;32m/orfeo/cephfs/scratch/area/evillegas/glm/dgxtorch/lib/python3.10/site-packages/transformers/image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/orfeo/cephfs/scratch/area/evillegas/glm/dgxtorch/lib/python3.10/site-packages/transformers/models/convnext/image_processing_convnext.py:281\u001b[0m, in \u001b[0;36mConvNextImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, crop_pct, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m size \u001b[38;5;241m=\u001b[39m get_size_dict(size, default_to_square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    279\u001b[0m validate_kwargs(captured_kwargs\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mkeys(), valid_processor_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_processor_keys)\n\u001b[0;32m--> 281\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmake_list_of_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_images(images):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n",
      "File \u001b[0;32m/orfeo/cephfs/scratch/area/evillegas/glm/dgxtorch/lib/python3.10/site-packages/transformers/image_utils.py:165\u001b[0m, in \u001b[0;36mmake_list_of_images\u001b[0;34m(images, expected_ndims)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image shape. Expected either \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_ndims\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_ndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, but got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         )\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax.ndarray, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'NoneType'>."
     ]
    }
   ],
   "source": [
    "main_inference(model_name, output_folder, emb_format, save_method, max_batches, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4aed7ba3-cc35-4cdb-b9f3-c87d98e99c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ds[\"train\"][\"img\"][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0ff80e-201d-47d3-bbd9-2c52cc0fe127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdc93caa9234e639d59e63d6b6a854e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/18.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ed6b9dfe2b4690bd32fe473af90944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/591753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ChristophSchuhmann/MS_COCO_2017_URL_TEXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93418bf5-72e3-4819-9979-bc1c1cf1cb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://images.cocodataset.org/train2017/000000391895.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['URL'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9c5a3-8699-4600-921c-40b1c8a880e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
